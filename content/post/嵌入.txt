我们通过前面几篇Transformers框架的技术文章，探讨了大模型的配置、分词器和BPE分词算法，通过这些信息和技术，可以把一段文本序列处理成一个Token（词元）列表。最终，通过词表可以为每个Token分配一个Token ID。到这里，大模型输入层的工作其实还没有结束，它还有关键的一步操作：嵌入（Embedding）。其中，词嵌入（Token Embeddings）是最基础也是最关键的步骤。

# 1. 嵌入（Embedding）

在人工智能领域，特别是在自然语言处理（NLP）和深度学习中，“嵌入”（Embedding）是一个至关重要的概念。它是指：**将离散的符号（如单词、字符或句子）映射到连续向量空间的过程**。这些向量通常被称为“**嵌入向量**”，这样计算机才能够更有效地理解和处理人类语言。

嵌入有几个核心概念：

- **高维向量表示**：每个符号（例如单词）被表示为一个固定维度的实数向量。这个向量的空间通常具有较高的维度（如100, 10000甚至更高），以便能够充分表达复杂的语义信息。
- **语义相似性**：在嵌入空间中，语义上相似的词汇往往具有接近的向量表示。比如，“猫”和“狗”的嵌入向量可能会非常接近，因为它们都是宠物动物；而“苹果”作为一个水果，其嵌入向量会与上述两者有一定距离。
- **上下文依赖**：目前嵌入模型，如BERT、Qwen等，不仅仅考虑单个词的静态意义，还会根据词语出现的具体上下文来动态调整其嵌入向量。这意味着同一个词在不同的句子中可能会有不同的表示，从而更好地捕捉其多义性和使用场景。比如，“这个苹果很甜。”和“我使用的是苹果手机。”这2个文本序列中，“苹果”的含义并不相同。

嵌入层（Embedding Layer）是神经网络模型中的一种特殊层，它的作用就是实现嵌入（Embedding），网络层（或隐藏层）的输入就是嵌入向量。

在大语言模型中，嵌入层本质上是一个嵌入矩阵，矩阵的行代表词表Token ID，矩阵的列则代表嵌入向量的维度。当给定一个Token时，嵌入层会根据Token ID，直接从嵌入矩阵中取出相应的嵌入向量。

# 2. Transformers 嵌入

接下来，我们以**Qwen2.5-1.8B**大模型为例，来看看嵌入：

首先，我们根据大模型`config.json`配置文件的**hidden_size**和**vocab_size**配置项，可以看出嵌入的维度是**1536**维，词表大小是**151936**个：

```json
{
	# ...省略...
	"hidden_size": 1536,
	"vocab_size": 151936
}
```

这样也就意味着，**Qwen2.5-1.8B**嵌入层尺寸是`(151936, 1536)`，简单理解就是嵌入矩阵有151936行，1536列。我们可以通过以下代码证实这一点：

```python
import os

from transformers import AutoTokenizer, AutoModel

# 本地模型目录
model_dir = os.path.join('D:', os.path.sep, 'ModelSpace', 'Qwen2.5', 'Qwen2.5-1.5B-Instruct')

# 初始化分词器和模型
tokenizer = AutoTokenizer.from_pretrained(
    model_dir,
    local_files_only=True,
)

model = AutoModel.from_pretrained(
    model_dir,
    torch_dtype='auto',
    device_map='auto',
    local_files_only=True,
)

# 查看嵌入矩阵和大小
embeddings = model.get_input_embeddings()
print(f'嵌入矩阵大小: {embeddings.weight.size()}')
# 输出：嵌入矩阵大小: torch.Size([151936, 1536])

# 查看整个嵌入矩阵
# print(embeddings.weight)
```

在Transformers框架的源代码中，我们可以看到嵌入矩阵的构建代码：

![Qwen2.5 词嵌入矩阵](31.jpg)

有了嵌入矩阵，我们可以进一步看看文本序列（如：`Hi, 你好~`）的嵌入向量内容：

```python
# 分词
word = 'Hi, 你好~'
tokens = tokenizer.tokenize(word)
print(f'{word} 分词: {tokens}')
# 输出：Hi, 你好~ 分词: ['Hi', ',', 'Ġ', 'ä½łå¥½', '~']

token_id = tokenizer.convert_tokens_to_ids(tokens)
print(f'{word} Toke ID: {token_id}')
# 输出：Hi, 你好~ Toke ID: [13048, 11, 220, 108386, 93]

# 获取该单词的嵌入向量
word_embedding = embeddings.weight[token_id]
print(f'{word} 的嵌入形状：{word_embedding.shape}')
# 输出：Hi, 你好~ 的嵌入形状：torch.Size([5, 1536])

print(f'{word} 的嵌入内容: {word_embedding}')
# 输出：
# Hi, 你好~ 的嵌入内容: tensor([[ 0.0261,  0.0048, -0.0043,  ...,  0.0193, -0.0493, -0.0020],
#         [-0.0303, -0.0159, -0.0107,  ..., -0.0198, -0.0020, -0.0129],
#         [-0.0236, -0.0254,  0.0325,  ..., -0.0317, -0.0082,  0.0137],
#         [ 0.0270,  0.0042,  0.0014,  ...,  0.0425, -0.0195,  0.0011],
#         [-0.0205, -0.0408, -0.0013,  ...,  0.0272, -0.0060,  0.0032]],
#        dtype=torch.bfloat16, grad_fn=<IndexBackward0>)
```

从最终的输出可以看出，文本序列`Hi, 你好~`分词成5个Token，对应有5个Token ID，最终他们的嵌入向量形状为`(5, 1536)`

# 3. Word2Vec 词嵌入技术

预训练大语言模型中（比如：Qwen2.5），词嵌入通常使用训练语料库进行学习来生成。其中，**Word2Vec** 是一种非常流行的词嵌入技术，自然语言处理（NLP）领域的重要工具之一。

Word2Vec 的核心思想是“分布假设”（Distributional Hypothesis），即词汇的意义可以通过其出现的上下文来推断。具体来说，如果两个词汇经常出现在相似的上下文中，那么它们可能具有相似的意义。Word2Vec 通过训练神经网络模型来学习词汇的分布式表示，使得这些表示能够反映词汇间的语义关系。

Word2Vec 提供了两种主要的模型架构，分别是 CBOW (Continuous Bag-of-Words) 和 Skip-gram。这两种模型的目标都是预测词汇或其上下文，但它们的工作方式略有不同：

| 特征 | CBOW | Skip-gram |
| --- | --- | --- |
| **目标** | 给定上下文，预测目标词汇 | 给定目标词汇，预测上下文词汇 |
| **训练速度** | 较快 | 较慢 |
| **对罕见词汇的表现** | 较差 | 较好 |
| **上下文窗口** | 固定，对称 | 固定或灵活 |
| **适用场景** | 常见词汇丰富的文本，快速原型开发，资源有限的环境 | 罕见词汇较多的文本，长尾分布的词汇，高质量词向量生成，多语言或多领域任务 |

接下来，我们用一个简单的实例，使用`gensim`库，训练一个 Word2Vec 模型。

## 安装包依赖

```shell
pip install gensim pandas
```

## 准备语料

假设我们有一个`train-data.txt`文本文件，每行代表一个句子。

```plaintext
# train-data.txt
I love programming in Python.
Python is a versatile language.
Machine learning is fascinating.
Deep learning is a subset of machine learning.
Natural language processing is a field of AI.
```

## 加载和预处理数据

使用`gensim.utils.simple_preprocess`来加载和预处理数据，通常包括：分词、去除标点符号、转换为小写等。

```python
import gensim
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

# 读取语料库
with open('train-data.txt', 'r', encoding='utf-8') as file:
    sentences = [line.strip() for line in file]

# 预处理：分词并去除标点符号
processed_sentences = [simple_preprocess(sentence) for sentence in sentences]

# 打印预处理后的句子
for sentence in processed_sentences:
    print(sentence)
```

我们可以看到打印的处理结果内容：

```plaintext
['i', 'love', 'programming', 'in', 'python']
['python', 'is', 'a', 'versatile', 'language']
['machine', 'learning', 'is', 'fascinating']
['deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning']
['natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'ai']
```

## 训练 Word2Vec 模型

使用`gensim.models.Word2Vec`类来训练 Word2Vec 模型。你可以选择使用 CBOW 或 Skip-gram 模型，并设置一些超参数，如嵌入维度、窗口大小、最小词频等。

```python
# 训练 Word2Vec 模型（使用 Skip-gram）
model = Word2Vec(
    sentences=processed_sentences,  # 输入数据
    vector_size=100,                # 嵌入维度
    window=5,                       # 上下文窗口大小
    min_count=1,                    # 忽略出现频率低于此值的词汇
    workers=4,                      # 使用多线程加速训练
    sg=1                            # 使用 Skip-gram 模型 (sg=0 表示使用 CBOW)
)

# 保存模型
model.save("word2vec.model")
```

## 加载预训练模型

我们可以使用`load`方法加载一个已经训练好了模型：

```python
# 加载预训练的 Word2Vec 模型
model = Word2Vec.load("word2vec.model")
```

## 使用 Word2Vec 模型

我们可以使用模型进行各种操作，如查找相似词汇、计算词汇之间的相似度、获取词汇的嵌入向量等。

### 查找相似词汇

```python
# 查找与 "python" 最相似的词汇
similar_words = model.wv.most_similar("python", topn=5)
print(similar_words)
```

输出结果：

```plaintext
[('programming', 0.9999999403953552),
 ('language', 0.9999999403953552),
 ('machine', 0.9999999403953552),
 ('learning', 0.9999999403953552),
 ('versatile', 0.9999999403953552)]
```

### 计算词汇之间的相似度

```python
# 计算 "machine" 和 "learning" 之间的相似度
similarity = model.wv.similarity("machine", "learning")
print(f"Similarity between 'machine' and 'learning': {similarity:.4f}")
```

输出结果：

```plaintext
Similarity between 'machine' and 'learning': 0.9999
```

### 获取词汇的嵌入向量

```python
# 获取 "python" 的嵌入向量
vector = model.wv["python"]
print(vector)
```

输出结果：

```plaintext
[ 0.007276  0.01151  -0.006436 ...  0.003214  0.002112  0.001234]
```
